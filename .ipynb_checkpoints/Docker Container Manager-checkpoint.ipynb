{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Container: 79faeaacec>, <Container: 7d213bfd6b>, <Container: 469a7395f9>]\n"
     ]
    }
   ],
   "source": [
    "# import Docker and get container infos\n",
    "import docker\n",
    "client = docker.from_env()\n",
    "running_containers = client.containers.list()\n",
    "names = [container.name for container in running_containers]\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Container: ca7bfbe852>\n",
      "2021-09-27 12:32:44,704 - INFO - no display found. Using non-interactive Agg backend\n",
      "usage: qc_pipeline.py [-h] [-o OUTDIR] [-p BASEPATH] [-c CONFIG] [-f]\n",
      "                      [-b BATCH] [-n NTHREADS] [-s]\n",
      "                      [input_pattern [input_pattern ...]]\n",
      "\n",
      "positional arguments:\n",
      "  input_pattern         input filename pattern (try: *.svs or\n",
      "                        target_path/*.svs ), or tsv file containing list of\n",
      "                        files to analyze\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -o OUTDIR, --outdir OUTDIR\n",
      "                        outputdir, default ./histoqc_output\n",
      "  -p BASEPATH, --basepath BASEPATH\n",
      "                        base path to add to file names, helps when producing\n",
      "                        data using existing output file as input\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        config file to use\n",
      "  -f, --force           force overwriting of existing files\n",
      "  -b BATCH, --batch BATCH\n",
      "                        break results file into subsets of this size\n",
      "  -n NTHREADS, --nthreads NTHREADS\n",
      "                        number of threads to launch\n",
      "  -s, --symlinkoff      turn OFF symlink creation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print HistoQC pipeline help\n",
    "hqc_container = [x for x in running_containers if x.name == 'hqc'][0]\n",
    "print(hqc_container)\n",
    "print(hqc_container.exec_run(\"python usr/local/src/qc_pipeline.py -h\").output.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config/hqc_config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-15c6a436cfcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfigparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"config/hqc_config.json\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mhqc_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config/hqc_config.json'"
     ]
    }
   ],
   "source": [
    "# CONFIG PARAMS to call qc_pipeline (stored in json/xml in the future)\n",
    "# two config parameters:\n",
    "# I:  call_config: config for command line call\n",
    "# II: calc_config: calculation/algorithm parameters: values for pipeline / algorithms\n",
    "\n",
    "import json\n",
    "import configparser\n",
    "\n",
    "with open(\"config/hqc_config.json\") as json_file:\n",
    "    hqc_config = json.loads(json_file.read())\n",
    "    \n",
    "input_path = \"usr/local/data/01CO001-760f15d2-444c-4deb-b133-62f3ca.svs\" # set input file\n",
    "output_path = hqc_config[\"output_path\"] # set output folder\n",
    "config_path = hqc_config[\"config_path\"] #choose config file\n",
    "base_path = \"-p \" + hqc_config[\"base_path\"] if len(hqc_config[\"base_path\"]) > 1 else \"\" # default in qc_pipeline: \"\" (empty string)\n",
    "force = \"-f\" if json.loads(hqc_config[\"force\"].lower()) else \"\" # force overwrite existing output files: default in qc_pipeline: False\n",
    "batch_size = \"-b\" + int(hqc_config[\"batch_size\"]) if int(hqc_config[\"batch_size\"]) > 0 else \"\" # default in config: 0 leads to default in qc_pipeline: float(\"inf\")\n",
    "n_threads = \"-n\" + int(hqc_config[\"n_threads\"]) if int(hqc_config[\"n_threads\"]) > 1 else \"\" # default in qc_pipeline: 1\n",
    "symlink_off = \"-s\" if json.loads(hqc_config[\"symlink_off\"].lower()) else \"\" # default in qc_pipeline: True\n",
    "\n",
    "copy_config_cmd = \"cp /usr/local/src/config_light.ini /usr/local/config\" \n",
    "print(hqc_container.exec_run(copy_config_cmd).output.decode('UTF-8'))\n",
    "\n",
    "config_path_host = \"C:\\\\Users\\\\phili\\\\Desktop\\\\Uni\\\\Master_Thesis\\\\Docker\\\\config\\\\config_light.ini\"\n",
    "config_path_client = \"usr/local/config/config_light.ini\"\n",
    "\n",
    "# call config:\n",
    "command_hqc = \"python usr/local/src/qc_pipeline.py {0} -o {1} -c {2} {3} {4} {5}\".format(input_path, output_path, config_path_client, n_threads, force, base_path)\n",
    "config_all = {}\n",
    "config_all[\"call_config\"] = command_hqc\n",
    "\n",
    "print(config_all)\n",
    "print(hqc_container.exec_run(command_hqc).output.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results from HQC using the index.html file and symlinks \n",
    "\n",
    "# create_symlink = \"ln -s /usr/local/src/data/output /usr/local/src/UserInterface/Data/output\"\n",
    "# print(hqc_container.exec_run(create_symlink).output.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Container: 596d12f50a>\n",
      "usage: create_patches_fp.py [-h] [--source SOURCE] [--step_size STEP_SIZE]\n",
      "                            [--patch_size PATCH_SIZE] [--patch] [--seg]\n",
      "                            [--stitch] [--no_auto_skip] [--save_dir SAVE_DIR]\n",
      "                            [--preset PRESET] [--patch_level PATCH_LEVEL]\n",
      "                            [--process_list PROCESS_LIST]\n",
      "\n",
      "seg and patch\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --source SOURCE       path to folder containing raw wsi image files\n",
      "  --step_size STEP_SIZE\n",
      "                        step_size\n",
      "  --patch_size PATCH_SIZE\n",
      "                        patch_size\n",
      "  --patch\n",
      "  --seg\n",
      "  --stitch\n",
      "  --no_auto_skip\n",
      "  --save_dir SAVE_DIR   directory to save processed data\n",
      "  --preset PRESET       predefined profile of default segmentation and filter\n",
      "                        parameters (.csv)\n",
      "  --patch_level PATCH_LEVEL\n",
      "                        downsample level at which to patch\n",
      "  --process_list PROCESS_LIST\n",
      "                        name of list of images to process with parameters\n",
      "                        (.csv)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print CLAM help (Create Patches from WSI (.SVS) and store in h5 structure)\n",
    "clam_container = [x for x in running_containers if x.name == 'clam'][0]\n",
    "print(clam_container)\n",
    "print(clam_container.exec_run(\"python usr/local/src/clam/create_patches_fp.py -h\").output.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  usr/local/src/data/01CO001-760f15d2-444c-4deb-b133-62f3ca.svs\n",
      "patch_save_dir:  usr/local/src/data/output/patches\n",
      "mask_save_dir:  usr/local/src/data/output/masks\n",
      "stitch_save_dir:  usr/local/src/data/output/stitches\n",
      "source : usr/local/src/data/01CO001-760f15d2-444c-4deb-b133-62f3ca.svs\n",
      "save_dir : usr/local/src/data/output\n",
      "patch_save_dir : usr/local/src/data/output/patches\n",
      "mask_save_dir : usr/local/src/data/output/masks\n",
      "stitch_save_dir : usr/local/src/data/output/stitches\n",
      "{'seg_params': {'seg_level': -1, 'sthresh': 8, 'mthresh': 7, 'close': 4, 'use_otsu': False, 'keep_ids': 'none', 'exclude_ids': 'none'}, 'filter_params': {'a_t': 100, 'a_h': 16, 'max_n_holes': 8}, 'patch_params': {'use_padding': True, 'contour_fn': 'four_pt'}, 'vis_params': {'vis_level': -1, 'line_thickness': 250}}\n",
      "\n",
      "\n",
      "progress: 0.00, 0/1\n",
      "processing usr/local/src/data/01CO001-760f15d2-444c-4deb-b133-62f3ca.svs\n",
      "SLIDE ID:  01CO001-760f15d2-444c-4deb-b133-62f3ca\n",
      "Creating patches for:  01CO001-760f15d2-444c-4deb-b133-62f3ca ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 1920 560 26737 40406\n",
      "Contour Area: 746454144.0\n",
      "Extracted 11482 coordinates\n",
      "segmentation took 1.3932900428771973 seconds\n",
      "patching took 1.907475471496582 seconds\n",
      "stitching took -1 seconds\n",
      "average segmentation time in s per slide: 1.3932900428771973\n",
      "average patching time in s per slide: 1.907475471496582\n",
      "average stiching time in s per slide: -1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract patchtes using CLAM\n",
    "\n",
    "# CLAM params (stored in json/xml in the future)\n",
    "patch_size = 128 # set patch size (128 needed for ARA-NET / 224 needed for VGG16 feature extraction)\n",
    "seg = \"--seg\"\n",
    "patch = \"--patch\"\n",
    "stitch = \"--stitch\"\n",
    "no_auto_skip = \"--no_auto_skip\"\n",
    "preset = \"--preset preset.csv\"\n",
    "patch_level = \"--patch_level 8\" # downsample level for patch calculation\n",
    "process_list = \"--process_list process_list.csv\"\n",
    "clam_command = \"python usr/local/src/clam/create_patches_fp.py --source {0} --save_dir {1} --patch_size {2} --seg --patch\".format(input_path, output_path, patch_size)\n",
    "print(clam_container.exec_run(clam_command).output.decode('UTF-8')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Container: ebb152c9c6>\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "usage: test_model.py [-h] [--model-path MODEL_PATH]\n",
      "                     [--input-images INPUT_IMAGES] [--measure {Entropy,BALD}]\n",
      "                     [--output-path OUTPUT_PATH] [--coords COORDS]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model-path MODEL_PATH\n",
      "                        Path to the model file\n",
      "  --input-images INPUT_IMAGES\n",
      "                        Path to the folder with images to test\n",
      "  --measure {Entropy,BALD}\n",
      "                        Uncertainty measure\n",
      "  --output-path OUTPUT_PATH\n",
      "                        Output folder for the results file\n",
      "  --coords COORDS       Bool if input folder containes coordinates in hdf5\n",
      "                        files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print ARA-NET help (Classify patches from CLAM, stored in h5 structure)\n",
    "ara_container = [x for x in running_containers if x.name == 'ara-net'][0]\n",
    "print(ara_container)\n",
    "\n",
    "# Run Ara help and print output\n",
    "print(ara_container.exec_run(\"python usr/local/src/src/test_model.py -h\").output.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS7534_1_2_1062344\n",
      "# Coords: 806\n",
      "SS7534_1_10_1062352\n",
      "# Coords: 1818\n",
      "SS7534_1_6_1062348\n",
      "# Coords: 11128\n",
      "SS7534_1_5_1062347\n",
      "# Coords: 6281\n",
      "SS7534_1_8_1062350\n",
      "# Coords: 10386\n",
      "SS7534_1_1_1062343\n",
      "# Coords: 4571\n",
      "SS7534_1_7_1062349\n",
      "# Coords: 5814\n"
     ]
    }
   ],
   "source": [
    "# parse output from shared volume\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "output_folder = \"/home/user/Documents/Master/Docker/data/output\"\n",
    "main_dict = {}\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for subfolder in dirs:\n",
    "        if subfolder.endswith(\".svs\"):\n",
    "            key = subfolder.replace(\".svs\", \"\")\n",
    "            print(key)\n",
    "            # HQC Data:\n",
    "            with open(output_folder + \"/\" + subfolder + '/scan_meta.json') as json_file:\n",
    "                data = json.loads(json.loads(json_file.read()))\n",
    "\n",
    "            # CLAM patches coords:\n",
    "            filename = output_folder + \"/patches/\" + key + \".h5\"\n",
    "            with h5py.File(filename, \"r\") as f:\n",
    "                coords = f[\"coords\"]\n",
    "                print(\"# Coords: %s\" % len(coords))\n",
    "                for idx, coord in enumerate(coords):\n",
    "                    data[\"patch_{0}_coords\".format(idx)] = coord\n",
    "                \n",
    "            \n",
    "            main_dict[key] = data\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15123, 13121])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dict[list(main_dict.keys())[0]][\"patch_800_coords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dfb2f1926d7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'main_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print(main_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
